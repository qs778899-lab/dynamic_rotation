"""
Author: zhangcongshe

Date: 2025/11/1

Version: 1.0
"""

import numpy as np
import cv2
import os
import sys
import time
import logging
import gc
import torch
import trimesh
from typing import Any
from spatialmath import SE3, SO3
sys.path.append("FoundationPose")
from estimater import *
try:
    from learning.training.predict_score import ScorePredictor
    from learning.training.predict_pose_refine import PoseRefinePredictor
except ImportError:
    pass # Assume they are available via estimater
from dino_mask import get_mask_from_GD
from qwen_mask import get_mask_from_qwen

# from calculate_grasp_pose_from_object_pose import calculate_grasp_pose_from_object_pose as choose_grasp_pose
from Utils import *
from datetime import datetime  # åœ¨ Utils import ä¹‹åé‡æ–°å¯¼å…¥ï¼Œé¿å…è¢«è¦†ç›–

'''
ä¾‹å¦‚grasp, lift, approach, 
twist, push, align, release, pull, nudge,ç­‰
'''
 # æ ¸å¿ƒæ§åˆ¶å‡½æ•° 



def detect_object_pose_using_foundation_pose(target:str,mesh_path,cam:dict[str, Any]):
    '''
    ä½¿ç”¨foundation poseæ¥æ£€æµ‹ç‰©ä½“ä½å§¿
    å…ˆæ‰¾åˆ°ç‰©ä½“åˆ†å‰²å›¾åƒï¼ˆgrounding + samï¼‰ï¼Œç„¶åä½¿ç”¨foundation poseæ¥æ£€æµ‹ç‰©ä½“ä½å§¿

    Args:
        target: è¦æ£€æµ‹çš„ç‰©ä½“
        mesh_path: ç‰©ä½“çš„meshè·¯å¾„
        cam: env.camera_main (dict containing 'cam' object and 'cam_k' matrix)
    Returns:
        center_pose: ç‰©ä½“ä½å§¿åœ¨ç›¸æœºåæ ‡
    '''

    debug = 0
    debug_dir = "debug"
    
    # Create timestamped save directory for debug images
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    save_dir = os.path.join("record_images_during_grasp", timestamp)
    os.makedirs(save_dir, exist_ok=True)
    
    set_logging_format()
    set_seed(0)
    mesh = trimesh.load(mesh_path)
    #? openscadçš„å•ä½æ˜¯mmï¼Œ ä½†æ˜¯è½¬ä¸ºobjæ–‡ä»¶åå•ä½åˆå˜æˆmï¼Œæ‰€ä»¥è¿˜æ˜¯éœ€è¦è½¬æ¢ï¼
    mesh.vertices /= 1000 #! å•ä½è½¬æ¢é™¤ä»¥1000
    # mesh.vertices /= 3
    to_origin, extents = trimesh.bounds.oriented_bounds(mesh)
    bbox = np.stack([-extents/2, extents/2], axis=0).reshape(2,3)
    
    # åˆå§‹åŒ–è¯„åˆ†å™¨å’Œå§¿æ€ä¼˜åŒ–å™¨
    scorer = ScorePredictor() 
    refiner = PoseRefinePredictor()
    glctx = dr.RasterizeCudaContext()
    # åˆ›å»ºFoundationPoseä¼°è®¡å™¨
    est = FoundationPose(model_pts=mesh.vertices, model_normals=mesh.vertex_normals, mesh=mesh, scorer=scorer, refiner=refiner, debug_dir=debug_dir, debug=debug, glctx=glctx)
    logging.info("estimator initialization done")
    
    cam_k = cam["cam_k"]
    camera = cam["cam"]

    center_pose = None

    try:
        frame_count = 0
        last_valid_pose = None  # ä¿å­˜ä¸Šä¸€æ¬¡æœ‰æ•ˆçš„pose
        
        while True:
            frames = camera.get_frames()
            if frames is None:
                continue
            color = frames['color']  #get_framesè·å–å½“å‰å¸§çš„æ‰€æœ‰æ•°æ®ï¼ˆRGBã€æ·±åº¦ç­‰ï¼‰
            depth = frames['depth']/1000

            color_path = os.path.join(save_dir, f"color_frame_{frame_count:06d}.png")
            print("befor foundation pose, color_shape: ", color.shape)
            cv2.imwrite(color_path, color)
            
            # æ¯éš”15å¸§è¿›è¡Œä¸€æ¬¡FoundationPoseæ£€æµ‹
            if frame_count % 15 == 0:
                mask = get_mask_from_GD(color, target)
                # mask = get_mask_from_qwen(color, target, model_path="/home/erlin/work/labgrasp/Qwen3-VL/Qwen3-VL-4B-Thinking", bbox_vis_path=os.path.join(save_dir, f"qwen_bbox_frame_{frame_count:06d}.png"))
            
                cv2.imshow("mask", mask)
                cv2.imshow("color", color)
                
                pose = est.register(K=cam_k, rgb=color, depth=depth, ob_mask=mask, iteration=50)
                print(f"ç¬¬{frame_count}å¸§æ£€æµ‹å®Œæˆï¼Œpose: {pose}")
                
                center_pose = pose@np.linalg.inv(to_origin) #! è¿™ä¸ªæ‰æ˜¯ç‰©ä½“ä¸­å¿ƒç‚¹çš„Pose
                
                vis = draw_posed_3d_box(cam_k, img=color, ob_in_cam=center_pose, bbox=bbox)
                vis = draw_xyz_axis(color, ob_in_cam=center_pose, scale=0.1, K=cam_k, thickness=3, transparency=0, is_input_rgb=True)
                cv2.imshow('object 6D pose', vis[...,::-1])
    
                mask_path = os.path.join(save_dir, f"mask_frame_{frame_count:06d}.png")
                vis_path = os.path.join(save_dir, f"vis_frame_{frame_count:06d}.png")
                cv2.imwrite(mask_path, mask)
                cv2.imwrite(vis_path, vis[...,::-1])    

                # input("break01")         
                # print("break01")   

                #? æ¸…ç†å†…å­˜
                torch.cuda.empty_cache()
                gc.collect()
 
                last_valid_pose = center_pose  # ä¿å­˜è¿™æ¬¡æ£€æµ‹çš„ç»“æœ
            else:
                # ä½¿ç”¨ä¸Šä¸€æ¬¡æ£€æµ‹çš„ç»“æœ
                center_pose = last_valid_pose
                # print(f"ç¬¬{frame_count}å¸§ä½¿ç”¨ä¸Šæ¬¡æ£€æµ‹ç»“æœ")
            
            print("center_pose_object: ", center_pose) 
            
            frame_count += 1
            cv2.waitKey(1)

            if center_pose is not None:
                break

    except KeyboardInterrupt:
        print("\n[ç”¨æˆ·ä¸­æ–­] æ”¶åˆ°ç»ˆæ­¢ä¿¡å·")
    finally:
        cv2.destroyAllWindows()

    return center_pose




def choose_grasp_pose(
    center_pose_array,
    dobot,
    T_ee_cam,
    z_xoy_angle,
    vertical_euler,
    grasp_tilt_angle,
    angle_threshold,
    T_tcp_ee_z,
    T_safe_distance,
    z_safe_distance,
    verbose=True
):
    """
    ä»ç‰©ä½“ä½å§¿è®¡ç®—æŠ“å–å§¿æ€ï¼ˆä¸æ‰§è¡Œç§»åŠ¨ï¼Œåªè®¡ç®—ï¼‰
    
    Args:
        center_pose_array: ç‰©ä½“ä¸­å¿ƒåœ¨ç›¸æœºåæ ‡ç³»ä¸­çš„ä½å§¿ (4x4 numpy array)
        dobot: Dobotæœºæ¢°è‡‚å¯¹è±¡
        T_ee_cam: ç›¸æœºåˆ°æœ«ç«¯æ‰§è¡Œå™¨çš„å˜æ¢çŸ©é˜µ (SE3å¯¹è±¡)
        z_xoy_angle: ç‰©ä½“ç»•zè½´æ—‹è½¬è§’åº¦ï¼Œç”¨äºè°ƒæ•´æŠ“å–æ¥è¿‘æ–¹å‘ (åº¦)
        vertical_euler: å‚ç›´å‘ä¸‹æŠ“å–çš„graspå§¿æ€çš„çš„æ¬§æ‹‰è§’ [rx, ry, rz] (åº¦)
        grasp_tilt_angle: å€¾æ–œæŠ“å–è§’åº¦ (åº¦)
        angle_threshold: zè½´å¯¹é½çš„è§’åº¦é˜ˆå€¼ (åº¦)
        T_tcp_ee_z: TCPåˆ°æœ«ç«¯æ‰§è¡Œå™¨çš„zè½´åç§» (ç±³)
        T_safe_distance: å®‰å…¨è·ç¦»ï¼Œé˜²æ­¢æŠ“å–æ—¶ä¸ç‰©ä½“ç¢°æ’ (ç±³)
        z_safe_distance: æœ€ç»ˆç§»åŠ¨æ—¶zæ–¹å‘çš„é¢å¤–å®‰å…¨è·ç¦» (æ¯«ç±³)
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
    
    Returns:
        grasp_pose: æŠ“å–ä½ç½®å’Œå§¿æ€ [x, y, z, rx, ry, rz] (æ¯«ç±³å’Œåº¦)
        T_base_ee_ideal: è®¡ç®—å¾—åˆ°çš„ç†æƒ³æœ«ç«¯æ‰§è¡Œå™¨ä½å§¿ (SE3å¯¹è±¡)
    """
    from scipy.spatial.transform import Rotation as R
    from grasp_utils import normalize_angle
    
    if vertical_euler is None:
        vertical_euler = [-180, 0, -90]
    
    if verbose:
        print("å¼€å§‹è®¡ç®—æŠ“å–å§¿æ€...")
    
    # ------è®¡ç®—åœ¨æœºå™¨äººåŸºç³»ä¸­çš„object pose------
    T_cam_object = SE3(center_pose_array, check=False)
    pose_now = dobot.get_pose()  # è·å–å½“å‰æœ«ç«¯æ‰§è¡Œå™¨ä½å§¿
    x_e, y_e, z_e, rx_e, ry_e, rz_e = pose_now
    
    # ä»å½“å‰æœºå™¨äººä½å§¿æ„é€ å˜æ¢çŸ©é˜µ T_base_ee
    T_base_ee = SE3.Rt(
        SO3.RPY([rx_e, ry_e, rz_e], unit='deg', order='zyx'),
        np.array([x_e, y_e, z_e]) / 1000.0,  # æ¯«ç±³è½¬ç±³
        check=False
    )
    
    # åæ ‡å˜æ¢é“¾: T_base_cam = T_base_ee * T_ee_cam
    T_base_cam = T_base_ee * T_ee_cam
    T_base_obj = T_base_cam * T_cam_object


    T_base_cam_array = np.array(T_base_cam, dtype=float)
    print("T_base_cam_array: ", T_base_cam_array)
    # print("break for cam")
    # input("break for cam")
    
    # ------object pose è°ƒæ•´------
    T_base_obj_array = np.array(T_base_obj, dtype=float)
    
    # 1. å°†object poseçš„zè½´è°ƒæ•´ä¸ºå‚ç›´æ¡Œé¢æœä¸Š
    current_rotation_matrix = T_base_obj_array[:3, :3]
    current_z_axis = current_rotation_matrix[:3, 2]
    target_z_axis = np.array([0, 0, 1])
    z_angle_error = np.degrees(np.arccos(np.clip(np.dot(current_z_axis, target_z_axis), -1.0, 1.0)))
    
    if z_angle_error > angle_threshold:
        rotation_axis = np.cross(current_z_axis, target_z_axis)
        rotation_axis_norm = np.linalg.norm(rotation_axis)
        
        if rotation_axis_norm < 1e-6:
            rotation_matrix_new = current_rotation_matrix
        else:
            rotation_axis = rotation_axis / rotation_axis_norm
            rotation_angle = np.arccos(np.clip(np.dot(current_z_axis, target_z_axis), -1.0, 1.0))
            K = np.array([
                [0, -rotation_axis[2], rotation_axis[1]],
                [rotation_axis[2], 0, -rotation_axis[0]],
                [-rotation_axis[1], rotation_axis[0], 0]
            ])
            R_z_align = np.eye(3) + np.sin(rotation_angle) * K + (1 - np.cos(rotation_angle)) * np.dot(K, K)
            rotation_matrix_new = np.dot(R_z_align, current_rotation_matrix)
        
        T_base_obj_aligned = np.eye(4)
        T_base_obj_aligned[:3, :3] = rotation_matrix_new
        T_base_obj_aligned[:3, 3] = T_base_obj_array[:3, 3]
        T_base_obj_final = SE3(T_base_obj_aligned, check=False)
    else:
        T_base_obj_final = T_base_obj
    
    # 2. å°†object poseçš„x,yè½´å¯¹é½åˆ°æœºå™¨äººåŸºåæ ‡ç³»çš„x,yè½´
    rotation_matrix_after_z = np.array(T_base_obj_final.R)
    current_x_axis = rotation_matrix_after_z[:3, 0]
    x_projected = np.array([current_x_axis[0], current_x_axis[1], 0])
    x_projected_norm = np.linalg.norm(x_projected)
    
    if x_projected_norm > 1e-6:
        x_projected = x_projected / x_projected_norm
        x_angle = np.arctan2(x_projected[1], x_projected[0])
        R_z_align_xy = np.array([
            [np.cos(-x_angle), -np.sin(-x_angle), 0],
            [np.sin(-x_angle), np.cos(-x_angle), 0],
            [0, 0, 1]
        ])
        rotation_matrix_final = np.dot(R_z_align_xy, rotation_matrix_after_z)
        T_base_obj_final_aligned = np.eye(4)
        T_base_obj_final_aligned[:3, :3] = rotation_matrix_final
        T_base_obj_final_aligned[:3, 3] = T_base_obj_array[:3, 3]
        T_base_obj_final = SE3(T_base_obj_final_aligned, check=False)
    
    # 3. å°†object poseç»•zè½´æ—‹è½¬æŒ‡å®šè§’åº¦
    T_base_obj_array = T_base_obj_final.A
    current_rotation = T_base_obj_array[:3, :3]
    current_translation = T_base_obj_array[:3, 3]
    
    theta = np.radians(z_xoy_angle)
    R_z = np.array([
        [np.cos(theta), -np.sin(theta), 0],
        [np.sin(theta), np.cos(theta), 0],
        [0, 0, 1]
    ])
    new_rotation = np.dot(R_z, current_rotation)
    T_base_obj_rotated = np.eye(4)
    T_base_obj_rotated[:3, :3] = new_rotation
    T_base_obj_rotated[:3, 3] = current_translation
    T_base_obj_final = SE3(T_base_obj_rotated, check=False)
    
    # ------è°ƒæ•´æŠ“å–å§¿æ€------
    tilted_euler = [vertical_euler[0] + grasp_tilt_angle, vertical_euler[1], vertical_euler[2]]
    
    R_target_xyz = R.from_euler('xyz', tilted_euler, degrees=True)
    T_object_grasp_ideal = SE3.Rt(
        SO3(R_target_xyz.as_matrix()),
        [0, 0, 0],
        check=False
    )
    
    # ------è®¡ç®—åœ¨æœºå™¨äººåŸºç³»ä¸­ï¼Œå¤¹çˆªgraspå³tcpçš„æŠ“å–å§¿æ€------
    T_base_grasp_ideal = T_base_obj_final * T_object_grasp_ideal
    
    # ------è®¡ç®—åœ¨æœºå™¨äººåŸºç³»ä¸­ï¼Œæœ«ç«¯æ‰§è¡Œå™¨eeçš„æŠ“å–å§¿æ€------
    T_tcp_ee = SE3(0, 0, T_tcp_ee_z)
    T_safe_distance_se3 = SE3(0, 0, T_safe_distance)
    T_base_ee_ideal = T_base_grasp_ideal * T_tcp_ee * T_safe_distance_se3
    
    # ------æå–ä½ç½®å’Œå§¿æ€------
    pos_mm = T_base_ee_ideal.t * 1000  # è½¬æ¢ä¸ºæ¯«ç±³
    rx, ry, rz = T_base_ee_ideal.rpy(unit='deg', order='zyx')
    rz = normalize_angle(rz)  # è§„èŒƒåŒ–åˆ°[-180, 180]åº¦
    
    pos_mm[2] += z_safe_distance  # æ·»åŠ zæ–¹å‘é¢å¤–å®‰å…¨è·ç¦»
    
    grasp_pose = [pos_mm[0], pos_mm[1], pos_mm[2], rx, ry, rz]

    pre_distance = 20
    pre_grasp_pose = [pos_mm[0], pos_mm[1], pos_mm[2]+ pre_distance, rx, ry, rz]
    
    if verbose:
        print(f"è®¡ç®—å®Œæˆ - ç›®æ ‡ä½ç½®: [{pos_mm[0]:.2f}, {pos_mm[1]:.2f}, {pos_mm[2]:.2f}] mm")
        print(f"è®¡ç®—å®Œæˆ - ç›®æ ‡å§¿æ€: rx={rx:.2f}Â°, ry={ry:.2f}Â°, rz={rz:.2f}Â°")
    
    return pre_grasp_pose, grasp_pose, T_base_ee_ideal

def choose_grasp_pose_with_cam3(
    center_pose_array,
    dobot,
    T_base_cam,
    z_xoy_angle,
    vertical_euler,
    grasp_tilt_angle,
    angle_threshold,
    T_tcp_ee_z,
    T_safe_distance,
    z_safe_distance,
    verbose=True
):
    """
    ä»ç‰©ä½“ä½å§¿è®¡ç®—æŠ“å–å§¿æ€ï¼ˆä¸æ‰§è¡Œç§»åŠ¨ï¼Œåªè®¡ç®—ï¼‰
    
    Args:
        center_pose_array: ç‰©ä½“ä¸­å¿ƒåœ¨ç›¸æœºåæ ‡ç³»ä¸­çš„ä½å§¿ (4x4 numpy array)
        dobot: Dobotæœºæ¢°è‡‚å¯¹è±¡
        T_ee_cam: ç›¸æœºåˆ°æœ«ç«¯æ‰§è¡Œå™¨çš„å˜æ¢çŸ©é˜µ (SE3å¯¹è±¡)
        z_xoy_angle: ç‰©ä½“ç»•zè½´æ—‹è½¬è§’åº¦ï¼Œç”¨äºè°ƒæ•´æŠ“å–æ¥è¿‘æ–¹å‘ (åº¦)
        vertical_euler: å‚ç›´å‘ä¸‹æŠ“å–çš„graspå§¿æ€çš„çš„æ¬§æ‹‰è§’ [rx, ry, rz] (åº¦)
        grasp_tilt_angle: å€¾æ–œæŠ“å–è§’åº¦ (åº¦)
        angle_threshold: zè½´å¯¹é½çš„è§’åº¦é˜ˆå€¼ (åº¦)
        T_tcp_ee_z: TCPåˆ°æœ«ç«¯æ‰§è¡Œå™¨çš„zè½´åç§» (ç±³)
        T_safe_distance: å®‰å…¨è·ç¦»ï¼Œé˜²æ­¢æŠ“å–æ—¶ä¸ç‰©ä½“ç¢°æ’ (ç±³)
        z_safe_distance: æœ€ç»ˆç§»åŠ¨æ—¶zæ–¹å‘çš„é¢å¤–å®‰å…¨è·ç¦» (æ¯«ç±³)
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
    
    Returns:
        grasp_pose: æŠ“å–ä½ç½®å’Œå§¿æ€ [x, y, z, rx, ry, rz] (æ¯«ç±³å’Œåº¦)
        T_base_ee_ideal: è®¡ç®—å¾—åˆ°çš„ç†æƒ³æœ«ç«¯æ‰§è¡Œå™¨ä½å§¿ (SE3å¯¹è±¡)
    """
    from scipy.spatial.transform import Rotation as R
    from grasp_utils import normalize_angle
    
    if vertical_euler is None:
        vertical_euler = [-180, 0, -90]
    
    if verbose:
        print("å¼€å§‹è®¡ç®—æŠ“å–å§¿æ€...")
    
    # ------è®¡ç®—åœ¨æœºå™¨äººåŸºç³»ä¸­çš„object pose------
    T_cam_object = SE3(center_pose_array, check=False)
    pose_now = dobot.get_pose()  # è·å–å½“å‰æœ«ç«¯æ‰§è¡Œå™¨ä½å§¿
    x_e, y_e, z_e, rx_e, ry_e, rz_e = pose_now
    
    # # ä»å½“å‰æœºå™¨äººä½å§¿æ„é€ å˜æ¢çŸ©é˜µ T_base_ee
    # T_base_ee = SE3.Rt(
    #     SO3.RPY([rx_e, ry_e, rz_e], unit='deg', order='zyx'),
    #     np.array([x_e, y_e, z_e]) / 1000.0,  # æ¯«ç±³è½¬ç±³
    #     check=False
    # )
    
    
    T_base_obj = T_base_cam * T_cam_object
    
    # ------object pose è°ƒæ•´------
    T_base_obj_array = np.array(T_base_obj, dtype=float)
    
    # 1. å°†object poseçš„zè½´è°ƒæ•´ä¸ºå‚ç›´æ¡Œé¢æœä¸Š
    current_rotation_matrix = T_base_obj_array[:3, :3]
    current_z_axis = current_rotation_matrix[:3, 2]
    target_z_axis = np.array([0, 0, 1])
    z_angle_error = np.degrees(np.arccos(np.clip(np.dot(current_z_axis, target_z_axis), -1.0, 1.0)))
    
    if z_angle_error > angle_threshold:
        rotation_axis = np.cross(current_z_axis, target_z_axis)
        rotation_axis_norm = np.linalg.norm(rotation_axis)
        
        if rotation_axis_norm < 1e-6:
            rotation_matrix_new = current_rotation_matrix
        else:
            rotation_axis = rotation_axis / rotation_axis_norm
            rotation_angle = np.arccos(np.clip(np.dot(current_z_axis, target_z_axis), -1.0, 1.0))
            K = np.array([
                [0, -rotation_axis[2], rotation_axis[1]],
                [rotation_axis[2], 0, -rotation_axis[0]],
                [-rotation_axis[1], rotation_axis[0], 0]
            ])
            R_z_align = np.eye(3) + np.sin(rotation_angle) * K + (1 - np.cos(rotation_angle)) * np.dot(K, K)
            rotation_matrix_new = np.dot(R_z_align, current_rotation_matrix)
        
        T_base_obj_aligned = np.eye(4)
        T_base_obj_aligned[:3, :3] = rotation_matrix_new
        T_base_obj_aligned[:3, 3] = T_base_obj_array[:3, 3]
        T_base_obj_final = SE3(T_base_obj_aligned, check=False)
    else:
        T_base_obj_final = T_base_obj
    
    # 2. å°†object poseçš„x,yè½´å¯¹é½åˆ°æœºå™¨äººåŸºåæ ‡ç³»çš„x,yè½´
    rotation_matrix_after_z = np.array(T_base_obj_final.R)
    current_x_axis = rotation_matrix_after_z[:3, 0]
    x_projected = np.array([current_x_axis[0], current_x_axis[1], 0])
    x_projected_norm = np.linalg.norm(x_projected)
    
    if x_projected_norm > 1e-6:
        x_projected = x_projected / x_projected_norm
        x_angle = np.arctan2(x_projected[1], x_projected[0])
        R_z_align_xy = np.array([
            [np.cos(-x_angle), -np.sin(-x_angle), 0],
            [np.sin(-x_angle), np.cos(-x_angle), 0],
            [0, 0, 1]
        ])
        rotation_matrix_final = np.dot(R_z_align_xy, rotation_matrix_after_z)
        T_base_obj_final_aligned = np.eye(4)
        T_base_obj_final_aligned[:3, :3] = rotation_matrix_final
        T_base_obj_final_aligned[:3, 3] = T_base_obj_array[:3, 3]
        T_base_obj_final = SE3(T_base_obj_final_aligned, check=False)
    
    # 3. å°†object poseç»•zè½´æ—‹è½¬æŒ‡å®šè§’åº¦
    T_base_obj_array = T_base_obj_final.A
    current_rotation = T_base_obj_array[:3, :3]
    current_translation = T_base_obj_array[:3, 3]
    
    theta = np.radians(z_xoy_angle)
    R_z = np.array([
        [np.cos(theta), -np.sin(theta), 0],
        [np.sin(theta), np.cos(theta), 0],
        [0, 0, 1]
    ])
    new_rotation = np.dot(R_z, current_rotation)
    T_base_obj_rotated = np.eye(4)
    T_base_obj_rotated[:3, :3] = new_rotation
    T_base_obj_rotated[:3, 3] = current_translation
    T_base_obj_final = SE3(T_base_obj_rotated, check=False)
    
    # ------è°ƒæ•´æŠ“å–å§¿æ€------
    tilted_euler = [vertical_euler[0] + grasp_tilt_angle, vertical_euler[1], vertical_euler[2]]
    
    R_target_xyz = R.from_euler('xyz', tilted_euler, degrees=True)
    T_object_grasp_ideal = SE3.Rt(
        SO3(R_target_xyz.as_matrix()),
        [0, 0, 0],
        check=False
    )
    
    # ------è®¡ç®—åœ¨æœºå™¨äººåŸºç³»ä¸­ï¼Œå¤¹çˆªgraspå³tcpçš„æŠ“å–å§¿æ€------
    T_base_grasp_ideal = T_base_obj_final * T_object_grasp_ideal
    
    # ------è®¡ç®—åœ¨æœºå™¨äººåŸºç³»ä¸­ï¼Œæœ«ç«¯æ‰§è¡Œå™¨eeçš„æŠ“å–å§¿æ€------
    T_tcp_ee = SE3(0, 0, T_tcp_ee_z)
    T_safe_distance_se3 = SE3(0, 0, T_safe_distance)
    T_base_ee_ideal = T_base_grasp_ideal * T_tcp_ee * T_safe_distance_se3
    
    # ------æå–ä½ç½®å’Œå§¿æ€------
    pos_mm = T_base_ee_ideal.t * 1000  # è½¬æ¢ä¸ºæ¯«ç±³
    rx, ry, rz = T_base_ee_ideal.rpy(unit='deg', order='zyx')
    rz = normalize_angle(rz)  # è§„èŒƒåŒ–åˆ°[-180, 180]åº¦
    
    pos_mm[2] += z_safe_distance  # æ·»åŠ zæ–¹å‘é¢å¤–å®‰å…¨è·ç¦»
    
    grasp_pose = [pos_mm[0], pos_mm[1], pos_mm[2], rx, ry, rz]

    pre_distance = 20
    pre_grasp_pose = [pos_mm[0], pos_mm[1], pos_mm[2]+ pre_distance, rx, ry, rz]
    
    if verbose:
        print(f"è®¡ç®—å®Œæˆ - ç›®æ ‡ä½ç½®: [{pos_mm[0]:.2f}, {pos_mm[1]:.2f}, {pos_mm[2]:.2f}] mm")
        print(f"è®¡ç®—å®Œæˆ - ç›®æ ‡å§¿æ€: rx={rx:.2f}Â°, ry={ry:.2f}Â°, rz={rz:.2f}Â°")
    
    return pre_grasp_pose, grasp_pose, T_base_ee_ideal


def detect_object_orientation(angle_camera, save_dir=None, max_attempts=100, verbose=True):
    """
    æ£€æµ‹ç‰©ä½“æ–¹å‘ï¼ˆä¾‹å¦‚ç»ç’ƒæ£’çš„æœå‘ï¼‰
    
    Args:
        angle_camera: ç”¨äºè§’åº¦æ£€æµ‹çš„ç›¸æœºå¯¹è±¡
        save_dir: ä¿å­˜æ£€æµ‹å›¾åƒçš„ç›®å½•ï¼ˆå¯é€‰ï¼‰
        max_attempts: æœ€å¤§å°è¯•æ¬¡æ•°
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
    
    Returns:
        avg_angle: æ£€æµ‹åˆ°çš„å¹³å‡è§’åº¦ï¼ˆåº¦ï¼‰
    """
    from calculate_grasp_pose_from_object_pose import detect_dent_orientation
    
    if verbose:
        print("\n" + "="*60)
        print("ğŸ” å¼€å§‹æ£€æµ‹ç‰©ä½“æ–¹å‘...")
        print("="*60)
    
    detected_angles = None
    avg_angle = 0.0
    detection_attempts = 0
    
    while True:
        detection_attempts += 1
        
        raw_image = angle_camera.get_current_frame()
        if raw_image is None:
            if verbose:
                print(f"ç¬¬{detection_attempts}æ¬¡å°è¯•: ç­‰å¾…ç›¸æœºæ•°æ®...")
            time.sleep(0.1)
            continue
        img_timestamp = time.time()
        
        if verbose:
            print(f"\nğŸ“· ç¬¬{detection_attempts}æ¬¡å°è¯•: æ£€æµ‹æ–°åŸå§‹å›¾åƒæ–¹å‘ (æ—¶é—´æˆ³: {img_timestamp:.2f})")
        
        detected_angles, avg_angle = detect_dent_orientation(raw_image, save_dir=save_dir)
        
        if detected_angles:
            if verbose:
                print(f"æˆåŠŸæ£€æµ‹åˆ°ç‰©ä½“æœå‘è§’åº¦: {detected_angles}, å¹³å‡: {avg_angle:.2f}Â°, ç»å¯¹å€¼: {abs(avg_angle):.2f}Â°")
                print("="*60)
            break
        else:
            if verbose:
                print("å½“å‰å›¾åƒæœªæ£€æµ‹åˆ°æ˜æ˜¾æ–¹å‘ç‰¹å¾ï¼Œç»§ç»­ç­‰å¾…...")
            time.sleep(0.1)
        
        # æœ€å¤§å°è¯•æ¬¡æ•°é™åˆ¶
        if detection_attempts >= max_attempts:
            if verbose:
                print(f"âš ï¸  è­¦å‘Š: è¾¾åˆ°æœ€å¤§å°è¯•æ¬¡æ•°({max_attempts}æ¬¡)ï¼Œä½¿ç”¨é»˜è®¤è§’åº¦")
            detected_angles = []
            avg_angle = 0.0
            break
    
    return avg_angle


def adjust_object_orientation(
    dobot,
    avg_angle,
    grasp_tilt_angle,
    x_adjustment=0,
    y_adjustment=0,
    z_adjustment=-15,
    move_speed=12,
    acceleration=1,
    wait_time=9.0,
    verbose=True
):
    """
    è°ƒæ•´ç‰©ä½“å§¿æ€è‡³å‚ç›´æ¡Œé¢å‘ä¸‹
    
    Args:
        dobot: Dobotæœºæ¢°è‡‚å¯¹è±¡
        avg_angle: æ£€æµ‹åˆ°çš„ç‰©ä½“å¹³å‡è§’åº¦ï¼ˆåº¦ï¼‰
        grasp_tilt_angle: æŠ“å–æ—¶çš„å€¾æ–œè§’åº¦ï¼ˆåº¦ï¼‰
        x_adjustment: xæ–¹å‘è°ƒæ•´é‡ï¼ˆæ¯«ç±³ï¼‰
        y_adjustment: yæ–¹å‘è°ƒæ•´é‡ï¼ˆæ¯«ç±³ï¼‰
        z_adjustment: zæ–¹å‘è°ƒæ•´é‡ï¼ˆæ¯«ç±³ï¼‰
        move_speed: ç§»åŠ¨é€Ÿåº¦
        acceleration: åŠ é€Ÿåº¦
        wait_time: ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
    
    Returns:
        pose_target: ç›®æ ‡å§¿æ€ [x, y, z, rx, ry, rz]
        pose_after_adjust: è°ƒæ•´åçš„å®é™…å§¿æ€ [x, y, z, rx, ry, rz]
    """
    import rospy
    
    if verbose:
        print("\nå¼€å§‹è°ƒæ•´ç‰©ä½“å§¿æ€è‡³å‚ç›´æ¡Œé¢å‘ä¸‹")

    wait_stable = rospy.Rate(1.0 / 1.0)
    wait_stable.sleep()
    
    pose_now = dobot.get_pose()
    delta_ee = abs(avg_angle) - grasp_tilt_angle
    
    # éœ€è¦è®©tcpæœå¤–æ—‹è½¬ï¼› grasp_tilt_angleä¸ºæ­£å€¼æ—¶ï¼Œtcpä¼šæœå¤–æ—‹è½¬ã€‚
    pose_target = [
        pose_now[0] + x_adjustment,
        pose_now[1] + y_adjustment,
        pose_now[2] + z_adjustment,
        pose_now[3] + delta_ee,
        pose_now[4],
        pose_now[5]
    ]
    
    dobot.move_to_pose(
        pose_target[0], pose_target[1], pose_target[2],
        pose_target[3], pose_target[4], pose_target[5],
        speed=move_speed,
        acceleration=acceleration
    )
    
    # ç­‰å¾…ç§»åŠ¨å®Œæˆ
    wait_rate = rospy.Rate(1.0 / wait_time)
    wait_rate.sleep()
    
    # éªŒè¯æ˜¯å¦åˆ°è¾¾ç›®æ ‡ä½ç½®
    pose_after_adjust = dobot.get_pose()
    
    if verbose:
        print(f"å§¿æ€è°ƒæ•´å®Œæˆ: Rx={pose_after_adjust[3]:.2f}Â° (ç›®æ ‡: {pose_target[3]:.2f}Â°)")
    
    return pose_target, pose_after_adjust


def detect_contact_with_surface(
    dobot,
    contact_camera,
    save_dir,
    sample_interval=0.1,
    move_step=3,
    max_steps=700,
    change_threshold=3,
    pixel_threshold=2,
    min_area=2,
    move_speed=5,
    acceleration=1,
    verbose=True
):
    """
    æ£€æµ‹ç‰©ä½“æ˜¯å¦è§¦ç¢°åˆ°æ¡Œé¢ï¼ˆé€šè¿‡ç›¸æœºå›¾åƒå˜åŒ–ï¼‰
    
    Args:
        dobot: Dobotæœºæ¢°è‡‚å¯¹è±¡
        contact_camera: ç”¨äºæ¥è§¦æ£€æµ‹çš„ç›¸æœºå¯¹è±¡
        save_dir: ä¿å­˜è°ƒè¯•å›¾åƒçš„ç›®å½•
        sample_interval: é‡‡æ ·é—´éš”ï¼ˆç§’ï¼‰
        move_step: æ¯æ­¥ä¸‹é™è·ç¦»ï¼ˆæ¯«ç±³ï¼‰
        max_steps: æœ€å¤§æ­¥æ•°
        change_threshold: å˜åŒ–é˜ˆå€¼ï¼ˆç™¾åˆ†æ¯”ï¼‰
        pixel_threshold: åƒç´ å˜åŒ–é˜ˆå€¼
        min_area: æœ€å°å˜åŒ–åŒºåŸŸ
        move_speed: ç§»åŠ¨é€Ÿåº¦
        acceleration: åŠ é€Ÿåº¦
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
    
    Returns:
        contact_detected: æ˜¯å¦æ£€æµ‹åˆ°æ¥è§¦ï¼ˆå¸ƒå°”å€¼ï¼‰
        steps_taken: å®é™…ä¸‹é™çš„æ­¥æ•°
        total_distance: æ€»ä¸‹é™è·ç¦»ï¼ˆæ¯«ç±³ï¼‰
    """
    import rospy
    
    if verbose:
        print("\nå¼€å§‹ç›‘æµ‹ç‰©ä½“ä¸æ¡Œé¢æ¥è§¦...")
    
    gray_debug_dir = os.path.join(save_dir, "gray_images_debug")
    os.makedirs(gray_debug_dir, exist_ok=True)
    
    rate = rospy.Rate(1.0 / sample_interval)
    rate.sleep()
    
    # è·å–åˆå§‹å›¾åƒ
    frame_before = None
    while frame_before is None:
        initial_frame = contact_camera.get_current_frame()
        if initial_frame is not None:
            frame_before = initial_frame
        else:
            if verbose:
                print("ç­‰å¾…åˆå§‹å›¾åƒ...")
            rospy.sleep(sample_interval)
    
    pose_current = dobot.get_pose()
    contact_detected = False
    steps_taken = 0
    
    for step in range(max_steps):
        wait = rospy.Rate(33)
        wait.sleep()
        
        # è·å–åŠ¨ä½œå‰å›¾åƒ
        frame_data_before = contact_camera.get_current_frame()
        if frame_data_before is None:
            if verbose:
                print(f"  æ­¥éª¤ {step+1}: ç­‰å¾…åŠ¨ä½œå‰å›¾åƒ...")
            rospy.sleep(sample_interval)
            continue
        frame_before = frame_data_before
        
        # å‘ä¸‹ç§»åŠ¨ä¸€å°æ­¥
        pose_current[2] -= move_step
        dobot.move_to_pose(
            pose_current[0], pose_current[1], pose_current[2],
            pose_current[3], pose_current[4], pose_current[5],
            speed=move_speed,
            acceleration=acceleration
        )
        
        # ç­‰å¾…å¹¶æŠ“å–åŠ¨ä½œåçš„æ–°å¸§ï¼Œè¿ç»­é«˜é¢‘é‡‡æ ·æ£€æµ‹
        frame_after = None
        has_change = False
        for _ in range(20):  # 0.1*20 = 2s
            rate.sleep()
            candidate_frame = contact_camera.get_current_frame()
            if candidate_frame is not None:
                frame_after = candidate_frame
                
                has_change = contact_camera.has_significant_change(
                    frame_before, frame_after,
                    change_threshold=change_threshold,
                    pixel_threshold=pixel_threshold,
                    min_area=min_area,
                    save_dir=gray_debug_dir,
                    step_num=step
                )
                
                if has_change:
                    break
        
        if frame_after is None:
            if verbose:
                print(f"  æ­¥éª¤ {step+1}: æœªæ”¶åˆ°æ–°å›¾åƒï¼Œç»§ç»­ç­‰å¾…...")
            continue
        
        if has_change:
            contact_detected = True
            steps_taken = step + 1
            if verbose:
                print(f"æ£€æµ‹åˆ°æ˜¾è‘—å˜åŒ–ï¼ç‰©ä½“å¯èƒ½å·²æ¥è§¦æ¡Œé¢ (æ­¥æ•°: {steps_taken}, ä¸‹é™: {steps_taken*move_step}mm)")
            break
        
        if verbose:
            print(f"  æ­¥éª¤ {step+1}/{max_steps}: æœªæ£€æµ‹åˆ°æ¥è§¦ï¼Œç»§ç»­ä¸‹é™...")
        steps_taken = step + 1
    
    if not contact_detected and verbose:
        print("è¾¾åˆ°å‚ç›´å‘ä¸‹æœ€å¤§ç§»åŠ¨è·ç¦»ï¼Œæœªæ£€æµ‹åˆ°æ˜æ˜¾å˜åŒ–")
    
    if verbose:
        print("ä¸‹é™æ£€æµ‹å®Œæˆ\n")
    
    total_distance = steps_taken * move_step
    return contact_detected, steps_taken, total_distance


def execute_grasp_action(
    grasp_pose,
    dobot,
    gripper,
    gripper_close_pos,
    move_speed=8,
    gripper_force=10,
    gripper_speed=30,
    verbose=True
):
    """
    æ‰§è¡ŒæŠ“å–åŠ¨ä½œ
    
    Args:
        grasp_pose: æŠ“å–ä½ç½®å’Œå§¿æ€ [x, y, z, rx, ry, rz]
        dobot: Dobotæœºæ¢°è‡‚å¯¹è±¡
        gripper: å¤¹çˆªå¯¹è±¡
        gripper_close_pos: å¤¹çˆªé—­åˆä½ç½®
        move_speed: ç§»åŠ¨é€Ÿåº¦
        gripper_force: å¤¹çˆªåŠ›é‡
        gripper_speed: å¤¹çˆªé€Ÿåº¦
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
    
    Returns:
        success: æ˜¯å¦æˆåŠŸæ‰§è¡ŒæŠ“å–
    """
    if verbose:
        print("\nå¼€å§‹æ‰§è¡ŒæŠ“å–åŠ¨ä½œ...")
        print(f"[æ‰§è¡Œ] ç›®æ ‡ä½ç½®: [{grasp_pose[0]:.2f}, {grasp_pose[1]:.2f}, {grasp_pose[2]:.2f}] mm")
        print(f"[æ‰§è¡Œ] ç›®æ ‡å§¿æ€: rx={grasp_pose[3]:.2f}Â°, ry={grasp_pose[4]:.2f}Â°, rz={grasp_pose[5]:.2f}Â°")
        print(f"[æ‰§è¡Œ] ç§»åŠ¨é€Ÿåº¦: {move_speed}")
    
    # ç§»åŠ¨åˆ°æŠ“å–ä½ç½®
    dobot.move_to_pose(
        grasp_pose[0], grasp_pose[1], grasp_pose[2],
        grasp_pose[3], grasp_pose[4], grasp_pose[5],
        speed=move_speed
    )
    
    if dobot.check_pose(grasp_pose[0], grasp_pose[1], grasp_pose[2]):
        if verbose:
            print("[æ‰§è¡Œ] åˆ°è¾¾æŒ‡å®šæŠ“å–ç‰©ä½“ä½ç½®")
    
    # æœ€ç»ˆä½ç½®
    final_pos = [grasp_pose[0], grasp_pose[1], grasp_pose[2]]
    dobot.move_to_pose(
        *final_pos,
        grasp_pose[3], grasp_pose[4], grasp_pose[5],
        speed=move_speed
    )
    
    if dobot.check_pose(*final_pos):
        gripper.control(gripper_close_pos, gripper_force, gripper_speed)
        print("å¤¹çˆªå¼€å§‹é—­åˆ")
        
        # ç­‰å¾…å¤¹çˆªåˆ°è¾¾ç›®æ ‡ä½ç½®
        timeout, interval = 5.0, 0.1
        elapsed = 0
        while elapsed < timeout:
            current = gripper.read_current_position()
            if current and abs(current[0] - gripper_close_pos) < 10:
                break
            time.sleep(interval)
            elapsed += interval
        
        if verbose:
            print("[å®Œæˆ] æŠ“å–æ“ä½œå®Œæˆ!")
        return True
    else:
        if verbose:
            print("[å¤±è´¥] æœªèƒ½åˆ°è¾¾æœ€ç»ˆæŠ“å–ä½ç½®")
        return False



  





# def grasp(
#     object, #è¦æŠ“å–çš„ç‰©ä½“
#     arm, #æœºæ¢°è‡‚
#     pre_grasp_dist, #æŠ“å–å‰è·ç¦»
#     grap_dis, #æŠ“å–å…·ä½“ä½ç½®ï¼ŒæŠ“å–ä½ç½®çš„zæ–¹å‘
#     gripper_pose#å¤¹çˆªå¼ å¼€é—­åˆçš„å°ºåº¦
# ):
#     # è®¡ç®—æŠ“å–å§¿æ€
#     pre_grasp_pose, grasp_pose = choose_grasp_pose()

#     action.move(pre_grasp_pose_dist)

#     action.move(grasp_pose)
#     pass
