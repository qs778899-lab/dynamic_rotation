import argparse
import os

import cv2
import numpy as np
import yaml
from sklearn.decomposition import PCA

# ROS imports
import rospy
from std_msgs.msg import Float64MultiArray
from sensor_msgs.msg import Image
from cv_bridge import CvBridge

from gs_sdk.gs_device import Camera, FastCamera
from gs_sdk.gs_reconstruct import Reconstructor
from normalflow.registration import normalflow, InsufficientOverlapError
from normalflow.utils import erode_contact_mask, gxy2normal, transform2pose
from normalflow.viz_utils import annotate_coordinate_system

"""
This script demonstrates real-time object tracking using normalflow with automatic reference frame generation.
The system automatically detects the principal axis of the contact object and sets up a coordinate system
with x-axis along the principal axis (kept in the right half of the image) and y-axis perpendicular to it.
"""

# --- Configuration Constants ---
DEFAULT_CALIB_MODEL_PATH = os.path.join(os.path.dirname(__file__), "models", "nnmodel.pth")
DEFAULT_CONFIG_PATH = os.path.join(os.path.dirname(__file__), "configs", "gsmini.yaml")

MIN_CONTACT_AREA_PIXELS = 500
BACKGROUND_IMAGE_COLLECTION_COUNT = 10

# Error thresholds for resetting the keyframe (values are in degrees for rotation, mm for translation)
RESET_ROT_THRESHOLD = 5.0
RESET_TRANS_THRESHOLD = 1.0

def init_ros_publisher():
    """Initialize ROS node and publishers for tracking data and object orientation image."""
    rospy.init_node('object_tracking_publisher', anonymous=True)
    #!ä¸€ä¸ªros nodeå‘é€ä¸‰ä¸ªros topic
    pub_tracking = rospy.Publisher('tracking_data', Float64MultiArray, queue_size=10)
    pub_image = rospy.Publisher('image_object_orientation', Image, queue_size=10)
    pub_raw_image = rospy.Publisher('raw_image', Image, queue_size=10)
    bridge = CvBridge()
    return pub_tracking, pub_image, pub_raw_image, bridge

def publish_tracking_data(publisher, angle_z_deg, b, x, y):
    """Publish angle_z_deg and b values via ROS."""
    if publisher is not None:
        msg = Float64MultiArray()
        msg.data = [angle_z_deg, b, x, y]
        publisher.publish(msg)
        
        # é†’ç›®çš„ROSæ•°æ®å‘é€æ‰“å°
        print("=" * 60)
        print("ğŸš€ ROS TOPIC PUBLISHED: tracking_data")
        print("=" * 60)
        print(f"ğŸ“Š è§’åº¦ (angle_z_deg): {angle_z_deg:.6f}Â°")
        print(f"ğŸ“ æˆªè· (b):           {b:.6f}")
        print(f"ğŸ“ Xåæ ‡ (x):          {x:.6f}")
        print(f"ğŸ“ Yåæ ‡ (y):          {y:.6f}")
        print("=" * 60)
        
        rospy.loginfo(f"Published tracking data - angle_z_deg: {angle_z_deg:.6f}, b: {b:.6f}, x: {x:.6f}, y: {y:.6f}")

def publish_image(publisher, bridge, image):
    """Publish image via ROS."""
    if publisher is not None and image is not None:
        try:
            img_msg = bridge.cv2_to_imgmsg(image, encoding="bgr8")
            publisher.publish(img_msg)
            
            # é†’ç›®çš„ROSå›¾åƒå‘é€æ‰“å°
            height, width = image.shape[:2]
            print("=" * 60)
            print("ğŸ–¼ï¸  ROS TOPIC PUBLISHED: image object_orientation")
            print("=" * 60)
            print(f"ğŸ“ å›¾åƒå°ºå¯¸: {width} x {height} pixels")
            print(f"ğŸ¨ ç¼–ç æ ¼å¼: bgr8")
            print(f"ğŸ’¾ æ•°æ®å¤§å°: {len(img_msg.data)} bytes")
            print("=" * 60)
            
        except Exception as e:
            print(f"âŒ ROSå›¾åƒå‘å¸ƒå¤±è´¥: {e}")
            rospy.logerr(f"Failed to publish image: {e}")

def publish_raw_image(publisher, bridge, image):
    """Publish raw image via ROS (without any annotations)."""
    if publisher is not None and image is not None:
        try:
            # å°è¯•ä½¿ç”¨cv_bridgeè½¬æ¢
            img_msg = bridge.cv2_to_imgmsg(image, encoding="bgr8")
            publisher.publish(img_msg)
            
            # é†’ç›®çš„ROSå›¾åƒå‘é€æ‰“å°
            height, width = image.shape[:2]
            print("=" * 60)
            print("ğŸ“· ROS TOPIC PUBLISHED: raw_image")
            print("=" * 60)
            print(f"ğŸ“ å›¾åƒå°ºå¯¸: {width} x {height} pixels")
            print(f"ğŸ¨ ç¼–ç æ ¼å¼: bgr8")
            print(f"ğŸ’¾ æ•°æ®å¤§å°: {len(img_msg.data)} bytes")
            print("=" * 60)
            
        except Exception as e:
            print(f"ros raw image å‘é€å¤±è´¥")
        
class ReferenceFrame:
    """Container for reference frame data"""
    def __init__(self, image, H, C, N, cx, cy, principal_angle=0.0):
        self.image = image
        self.H = H
        self.C = C
        self.N = N
        self.cx = cx
        self.cy = cy
        self.principal_angle = principal_angle  
        
    def get_coordinate_system_vectors(self):
        """Get coordinate system unit vectors for visualization"""
        cos_a = np.cos(self.principal_angle)
        sin_a = np.sin(self.principal_angle)

        x_vec = np.array([cos_a, sin_a])
        y_vec = np.array([-sin_a, cos_a])  
        
        return np.array([x_vec, y_vec, [0, 0]])
        
    def get_coordinate_transform_matrix(self):
        """Get coordinate transformation matrix"""
        T = np.eye(4)
        cos_a = np.cos(self.principal_angle)
        sin_a = np.sin(self.principal_angle)
        T[0, 0] = cos_a
        T[0, 1] = -sin_a
        T[1, 0] = sin_a
        T[1, 1] = cos_a
        return T

def generate_reference_from_first_frame(image_curr, reconstructor, ppmm, min_contact_area=500):
    """
    ä»ç¬¬ä¸€å¸§å›¾åƒç”Ÿæˆå•ä¸ªå‚è€ƒå›¾åƒï¼Œè‡ªåŠ¨æ£€æµ‹é•¿æ¡ç‰©ä½“çš„é•¿è¾¹æ–¹å‘ä½œä¸ºåˆå§‹xæ–¹å‘ã€‚
    
    Args:
        image_curr: å½“å‰å¸§å›¾åƒ
        reconstructor: é‡å»ºå™¨å¯¹è±¡
        ppmm: åƒç´ åˆ°æ¯«ç±³çš„è½¬æ¢æ¯”ä¾‹
        min_contact_area: æœ€å°æ¥è§¦é¢ç§¯é˜ˆå€¼
        
    Returns:
        ReferenceFrame: å•ä¸ªå‚è€ƒå¸§å¯¹è±¡ï¼Œæˆ–è€… None: å¦‚æœæ£€æµ‹å¤±è´¥
    """
    
    # è·å–è¡¨é¢ä¿¡æ¯
    G_curr, H_curr, C_curr = reconstructor.get_surface_info(image_curr, ppmm)
    C_curr = erode_contact_mask(C_curr)
    
    # æ£€æŸ¥æ¥è§¦é¢ç§¯
    if np.sum(C_curr) < min_contact_area:
        print(f"æ¥è§¦é¢ç§¯å¤ªå° ({np.sum(C_curr)} < {min_contact_area} pixels)")
        return None
    
    # è®¡ç®—æ³•å‘é‡
    N_curr = gxy2normal(G_curr)
    
    # è®¡ç®—æ¥è§¦åŒºåŸŸçš„è´¨å¿ƒ
    cx_curr, cy_curr = calculate_contact_centroid(C_curr)
    if cx_curr is None or cy_curr is None:
        print("æ— æ³•è®¡ç®—æ¥è§¦åŒºåŸŸè´¨å¿ƒ")
        return None
    
    # æ£€æµ‹é•¿æ¡ç‰©ä½“çš„ä¸»è½´æ–¹å‘
    principal_angle = detect_principal_axis(C_curr)
    if principal_angle is None:
        print("æ— æ³•æ£€æµ‹åˆ°ä¸»è½´æ–¹å‘")
        return None
    
    # è°ƒæ•´è§’åº¦ï¼Œç¡®ä¿xè½´åœ¨1,4è±¡é™ï¼ˆå³åŠå¹³é¢ï¼‰
    adjusted_angle = adjust_angle_to_right_half(principal_angle)
    
    print(f"æ£€æµ‹åˆ°çš„ä¸»è½´è§’åº¦: {np.degrees(principal_angle):.2f}Â°")
    print(f"è°ƒæ•´åçš„è§’åº¦: {np.degrees(adjusted_angle):.2f}Â°")
    
    # ç”Ÿæˆå•ä¸ªå‚è€ƒå¸§ï¼Œé•¿è¾¹æ–¹å‘ä¸ºxè½´
    reference = ReferenceFrame(
        image=image_curr.copy(),
        H=H_curr.copy(),
        C=C_curr.copy(),
        N=N_curr.copy(),
        cx=cx_curr,
        cy=cy_curr,
        principal_angle=adjusted_angle
    )
    
    return reference

def detect_principal_axis(contact_mask):
    """
    æ£€æµ‹æ¥è§¦åŒºåŸŸçš„ä¸»è½´æ–¹å‘ï¼ˆé•¿è¾¹æ–¹å‘ï¼‰ã€‚
    
    Args:
        contact_mask: æ¥è§¦æ©ç ï¼ˆäºŒå€¼å›¾åƒï¼‰
        
    Returns:
        float: ä¸»è½´è§’åº¦ï¼ˆå¼§åº¦ï¼‰ï¼Œå¦‚æœæ£€æµ‹å¤±è´¥åˆ™è¿”å›None
    """
    
    # æ‰¾åˆ°æ¥è§¦åŒºåŸŸçš„è½®å»“
    binary_mask = (contact_mask * 255).astype(np.uint8)
    contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    
    if not contours:
        return None
    
    # è·å–æœ€å¤§è½®å»“
    largest_contour = max(contours, key=cv2.contourArea)
    
    # æ–¹æ³•1ï¼šä½¿ç”¨è½®å»“çš„æœ€å°å¤–æ¥çŸ©å½¢
    rect = cv2.minAreaRect(largest_contour)
    angle_deg = rect[2]  # è§’åº¦ï¼ˆåº¦ï¼‰
    width, height = rect[1]
    
    # ç¡®ä¿è§’åº¦å¯¹åº”é•¿è¾¹æ–¹å‘
    if width < height:
        angle_deg += 90
    
    # è½¬æ¢ä¸ºå¼§åº¦
    angle_rad = np.radians(angle_deg)
    
    # æ–¹æ³•2ï¼šä½¿ç”¨PCAè¿›è¡ŒéªŒè¯
    # è·å–è½®å»“ç‚¹
    points = largest_contour.reshape(-1, 2)
    
    # ä½¿ç”¨PCAæ‰¾åˆ°ä¸»æˆåˆ†
    pca = PCA(n_components=2)
    pca.fit(points)
    
    # ç¬¬ä¸€ä¸»æˆåˆ†å¯¹åº”æœ€å¤§æ–¹å·®æ–¹å‘ï¼ˆé€šå¸¸æ˜¯é•¿è¾¹ï¼‰
    principal_component = pca.components_[0]
    pca_angle = np.arctan2(principal_component[1], principal_component[0]) #np.arctan2(y,x)
    
    # æ¯”è¾ƒä¸¤ç§æ–¹æ³•çš„ç»“æœ
    angle_diff = abs(angle_rad - pca_angle)
    if angle_diff > np.pi/2:
        angle_diff = np.pi - angle_diff
    
    # # å¦‚æœä¸¤ç§æ–¹æ³•å·®å¼‚è¾ƒå¤§ï¼Œä¼˜å…ˆä½¿ç”¨PCAç»“æœ
    # if angle_diff > np.pi/4:
    #     print(f"è½®å»“æ–¹æ³•è§’åº¦: {np.degrees(angle_rad):.2f}Â°, PCAæ–¹æ³•è§’åº¦: {np.degrees(pca_angle):.2f}Â°")
    #     print("ä½¿ç”¨PCAæ–¹æ³•ç»“æœ")
    #     return pca_angle
    # else:
    #     return angle_rad

    return pca_angle

def adjust_angle_to_right_half(angle_rad):
    """
    è°ƒæ•´è§’åº¦ï¼Œç¡®ä¿xè½´æ–¹å‘åœ¨å›¾åƒå³åŠå¹³é¢ã€‚
    å°†è§’åº¦æ˜ å°„åˆ°[-Ï€/2, Ï€/2]èŒƒå›´ï¼Œä¿æŒxè½´æŒ‡å‘å³ä¾§ã€‚
    """
    # å°†è§’åº¦æ ‡å‡†åŒ–åˆ°[-Ï€, Ï€]
    angle_rad = (angle_rad + np.pi) % (2 * np.pi) - np.pi
    
    # å¦‚æœè§’åº¦åœ¨å·¦åŠå¹³é¢ï¼ˆ|angle| > Ï€/2ï¼‰ï¼Œè°ƒæ•´åˆ°å³åŠå¹³é¢
    if angle_rad > np.pi / 2:
        angle_rad = angle_rad - np.pi
    elif angle_rad < -np.pi / 2:
        angle_rad = angle_rad + np.pi
        
    return angle_rad

def get_coordinate_vectors_from_angle(angle_rad, scale=1.0):
    """
    æ ¹æ®ç»™å®šè§’åº¦è·å–åæ ‡ç³»å‘é‡ã€‚
    è¿”å›ç”¨äºå¯è§†åŒ–çš„xå’Œyå•ä½å‘é‡ã€‚
    """
    # xè½´æ–¹å‘
    x_vec = np.array([np.cos(angle_rad), np.sin(angle_rad)]) * scale
    # yè½´æ–¹å‘ï¼ˆå‚ç›´äºxè½´ï¼‰
    y_vec = np.array([-np.sin(angle_rad), np.cos(angle_rad)]) * scale
    
    return np.array([x_vec, y_vec, [0, 0]])

def parse_arguments():
    """Parses command-line arguments."""
    parser = argparse.ArgumentParser(
        description="Real-time tracking the object using tactile sensors with automatic reference detection."
    )
    parser.add_argument(
        "-b",
        "--calib_model_path",
        type=str,
        help=f"Directory where calibration data and model are stored (default: {DEFAULT_CALIB_MODEL_PATH})",
        default=DEFAULT_CALIB_MODEL_PATH,
    )
    parser.add_argument(
        "-c",
        "--config_path",
        type=str,
        help=f"Path of the configuration file for the GelSight sensor (default: {DEFAULT_CONFIG_PATH})",
        default=DEFAULT_CONFIG_PATH,
    )
    parser.add_argument(
        "-s",
        "--streamer",
        type=str,
        choices=["opencv", "ffmpeg"],
        help="The sensor streamer. 'opencv' for 'gs_device.Camera' and 'ffmpeg' for 'gs_device.FastCamera'.",
        default="opencv",
    )
    parser.add_argument(
        "-d",
        "--device",
        type=str,
        choices=["cpu", "cuda"],
        help="The device to run the neural network model that predicts the normal map (default: 'cpu')",
        default="cpu",
    )
    return parser.parse_args()

def load_sensor_config(config_path):
    """Loads sensor configuration from a YAML file."""
    try:
        with open(config_path, "r") as f:
            config = yaml.safe_load(f)
        required_keys = ["device_name", "ppmm", "imgh", "imgw", "raw_imgh", "raw_imgw", "framerate"]
        if not all(key in config for key in required_keys):
            raise ValueError(f"Missing one or more required keys in config file: {required_keys}")
        return config
    except FileNotFoundError:
        print(f"Error: Configuration file not found at '{config_path}'")
        exit()
    except yaml.YAMLError as e:
        print(f"Error parsing YAML configuration file: {e}")
        exit()
    except ValueError as e:
        print(f"Error in configuration file: {e}")
        exit()

def connect_to_sensor(streamer_type, config):
    """Connects to the GelSight sensor based on streamer type."""
    device_name = config["device_name"]
    imgh, imgw = config["imgh"], config["imgw"]
    raw_imgh, raw_imgw = config["raw_imgh"], config["raw_imgw"]
    framerate = config["framerate"]

    if streamer_type == "opencv":
        device = Camera(device_name, imgh, imgw)
    elif streamer_type == "ffmpeg":
        device = FastCamera(device_name, imgh, imgw, raw_imgh, raw_imgw, framerate)
    else:
        raise ValueError(f"Unknown streamer type: {streamer_type}")

    try:
        device.connect()
        print(f"Successfully connected to GelSight sensor: {device_name}")
        return device
    except Exception as e:
        print(f"Error connecting to GelSight sensor: {e}")
        exit()

def collect_background_images(device, reconstructor, count=BACKGROUND_IMAGE_COLLECTION_COUNT):
    """Collects and loads background images for reconstruction."""
    print(f"Collecting {count} background images, please wait ...")
    bg_images = []
    for i in range(count):
        image = device.get_image()
        if image is None:
            print(f"Warning: Could not get image from device during background collection (attempt {i+1}/{count}).")
            continue
        bg_images.append(image)
    
    if not bg_images:
        print("Error: Failed to collect any background images. Cannot proceed with reconstruction.")
        device.release()
        exit()

    bg_image = np.mean(bg_images, axis=0).astype(np.uint8)
    reconstructor.load_bg(bg_image)
    print("Done with background collection.")

def calculate_contact_centroid(contact_mask, debug_image=None):
    """Calculate the centroid of the contact area from the contact mask."""
    binary_mask = (contact_mask * 255).astype(np.uint8)
    contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    
    if not contours:
        return None, None
    
    # Get the largest contour
    largest_contour = max(contours, key=cv2.contourArea)
    M = cv2.moments(largest_contour)
    
    if M["m00"] == 0:
        return None, None
    
    cx = int(M["m10"] / M["m00"])
    cy = int(M["m01"] / M["m00"])

    if debug_image is not None:
        cv2.circle(debug_image, (cx, cy), 5, (0, 0, 255), -1)
        cv2.drawContours(debug_image, [largest_contour], -1, (255, 255, 0), 2)
        cv2.imshow("Debug Centroid", debug_image)
    
    return cx, cy

def realtime_object_tracking():
    args = parse_arguments()

    # Initialize ROS publishers
    ros_pub_tracking, ros_pub_image, ros_pub_raw_image, ros_bridge = init_ros_publisher()

    # Load configuration
    config = load_sensor_config(args.config_path)
    ppmm = config["ppmm"] # This is mm/pixel as clarified by user
    imgh, imgw = config["imgh"], config["imgw"]

    # Connect to the sensor and the reconstructor
    device = connect_to_sensor(args.streamer, config)
    recon = Reconstructor(args.calib_model_path, device=args.device)

    # Collect background images
    collect_background_images(device, recon)

    # ç­‰å¾…ç¬¬ä¸€æ¬¡æ¥è§¦å¹¶ç”Ÿæˆå‚è€ƒå›¾åƒ
    print("ç­‰å¾…æ¥è§¦ï¼Œå°†ä»ç¬¬ä¸€å¸§è‡ªåŠ¨ç”Ÿæˆå‚è€ƒå›¾åƒ...")
    reference = None
    is_reset = False
    while True:
        first_frame = device.get_image()
        if first_frame is None:
            continue
        
        reference = generate_reference_from_first_frame(first_frame, recon, ppmm, MIN_CONTACT_AREA_PIXELS)
        if reference is not None:
            print("æˆåŠŸç”Ÿæˆå‚è€ƒå›¾åƒï¼")
            break
        else:
            print("ç­‰å¾…æœ‰æ•ˆæ¥è§¦...")
            # åªæ˜¾ç¤ºå·¦ä¾§å›¾åƒï¼Œæ˜¾ç¤º"waiting for contact"
            display_image = np.zeros((imgh, imgw * 2, 3), dtype=np.uint8)
            cv2.putText(
                display_image,
                "Waiting for Contact",
                (imgw // 2, imgh // 2),
                cv2.FONT_HERSHEY_SIMPLEX,
                1.0,
                (255, 255, 255),
                2,
            )
            cv2.imshow("frame", display_image)
            if cv2.waitKey(1) == ord('q'):
                device.release()
                exit()

    # Function to initialize/reset tracking parameters
    def reset_tracking_parameters():
        """Reset all tracking parameters to initial state for a new tracking session or after contact loss."""
        return (
            reference.C.copy(),   # C_ref: Contact mask of current reference frame
            reference.H.copy(),   # H_ref: Height map of current reference frame
            reference.N.copy(),   # N_ref: Normal map of current reference frame
            None,                 # C_prev: Contact mask of previous frame
            None,                 # H_prev: Height map of previous frame
            None,                 # N_prev: Normal map of previous frame
            np.eye(4),            # prev_T_ref: Transformation from previous frame to current reference frame
            np.eye(4),            # start_T_ref: Cumulative transformation from the *initial* reference frame to current reference frame
            False,                # has_contact_history: Flag to check if there was previous contact
        )

    # Initialize tracking variables
    C_ref, H_ref, N_ref, C_prev, H_prev, N_prev, \
    prev_T_ref, start_T_ref, has_contact_history = reset_tracking_parameters()

    # Add flags for angle tracking
    angle_offset = 0.0
    first_frame_angle_adjusted = False
    first_frame_after_contact_established = True

    # Real-time object tracking loop
    print("\nStarting real-time object tracking with automatic reference detection. Press any key to quit.\n")

    is_running = True
    while is_running:
        image_curr = device.get_image()
        if image_curr is None:
            print("Warning: Could not get image from device. Skipping frame.")
            key = cv2.waitKey(1)
            if key != -1:
                is_running = False
            continue

        G_curr, H_curr, C_curr = recon.get_surface_info(image_curr, ppmm)
        C_curr = erode_contact_mask(C_curr)

        # No sufficient contact in current frame
        if np.sum(C_curr) < MIN_CONTACT_AREA_PIXELS:
            
            if has_contact_history: #clean origin image
                print(f"Contact lost (area < {MIN_CONTACT_AREA_PIXELS} pixels) - resetting tracking parameters.")
                has_contact_history = False
                # é‡æ–°ç”Ÿæˆå‚è€ƒå›¾åƒ
                print("ç­‰å¾…é‡æ–°æ¥è§¦...")
                reference = None
                while True:
                    new_frame = device.get_image()
                    if new_frame is None:
                        continue
                    
                    reference = generate_reference_from_first_frame(new_frame, recon, ppmm, MIN_CONTACT_AREA_PIXELS)
                    if reference is not None:
                        print("é‡æ–°ç”Ÿæˆå‚è€ƒå›¾åƒæˆåŠŸï¼")
                        C_ref, H_ref, N_ref, C_prev, H_prev, N_prev, \
                        prev_T_ref, start_T_ref, has_contact_history = reset_tracking_parameters()
                        angle_offset = 0.0
                        first_frame_angle_adjusted = False
                        break
                    else:
                        # åªæ˜¾ç¤ºå·¦ä¾§å›¾åƒï¼Œæ˜¾ç¤º"waiting for contact"
                        display_image = np.zeros((imgh, imgw * 2, 3), dtype=np.uint8)
                        cv2.putText(
                            display_image,
                            "Waiting for Contact",
                            (imgw // 2, imgh // 2),
                            cv2.FONT_HERSHEY_SIMPLEX,
                            1.0,
                            (255, 255, 255),
                            2,
                        )
                        cv2.imshow("frame", display_image)
                        if cv2.waitKey(1) != -1:
                            device.release()
                            exit()
                continue
            display_image = np.zeros((imgh, imgw * 2, 3), dtype=np.uint8)
            cv2.putText(
                display_image,
                "Waiting for Contact",
                (imgw // 2, imgh // 2),
                cv2.FONT_HERSHEY_SIMPLEX,
                1.0,
                (255, 255, 255),
                2,
            )
            cv2.imshow("frame", display_image)
            key = cv2.waitKey(1)
            if key != -1:
                is_running = False
            continue

        # Mark that we now have contact
        has_contact_history = True
        N_curr = gxy2normal(G_curr)

        # Calculate current frame's centroid directly from contact mask
        cx_curr, cy_curr = calculate_contact_centroid(C_curr)
        if cx_curr is None or cy_curr is None:
            print("Warning: Could not calculate centroid of current frame. Skipping frame.")
            cv2.imshow("frame", image_curr)
            key = cv2.waitKey(1)
            if key != -1:
                is_running = False
            continue

        curr_T_ref = None
        is_reset_due_to_error = False
        max_iteration_tag = False

        try:
            # Attempt to track from current reference frame
            curr_T_ref, max_iteration_tag = normalflow(
                N_ref,
                C_ref,
                H_ref,
                N_curr,
                C_curr,
                H_curr,
                prev_T_ref,
                ppmm,
                5000,
                verbose=True
            )
        except InsufficientOverlapError:
            print("Insufficient overlap with current reference frame. Attempting reset.")
            is_reset_due_to_error = True
            if N_prev is None: 
                continue

        # Skip first frame after contact re-establishment if iteration count is bad
        if first_frame_after_contact_established:
            if max_iteration_tag == True:
                print(f"Skipping first frame after contact re-establishment due to bad iteration count.")
                has_contact_history = False
                cv2.imshow("frame", image_curr)
                key = cv2.waitKey(1)
                if key != -1:
                    is_running = False
                continue

        # Reset reference frame (set a new keyframe) if needed
        if N_prev is not None: # not first frame
            curr_T_prev = None
            try:
                # Calculate transformation from previous frame to current frame
                curr_T_prev, _ = normalflow(
                    N_prev,
                    C_prev,
                    H_prev,
                    N_curr,
                    C_curr,
                    H_curr,
                    np.eye(4),
                    ppmm,
                )
            except InsufficientOverlapError:
                print("Reset failed: Previous frame also has insufficient overlap with current. Skipping frame.")
                cv2.imshow("frame", image_curr)
                key = cv2.waitKey(1)
                if key != -1:
                    is_running = False
                continue

            # If curr_T_ref was successfully calculated, check for large errors against curr_T_prev
            if curr_T_ref is not None and not is_reset_due_to_error:
                T_error = np.linalg.inv(curr_T_ref) @ curr_T_prev @ prev_T_ref
                pose_error = transform2pose(T_error)
                rot_error = np.linalg.norm(pose_error[3:])
                trans_error = np.linalg.norm(pose_error[:3])

                if rot_error > RESET_ROT_THRESHOLD or trans_error > RESET_TRANS_THRESHOLD:
                    print(f"Large pose error detected (Rot: {rot_error:.2f} deg, Trans: {trans_error:.2f} m). Resetting reference frame.")
                    is_reset_due_to_error = True
            
            # If a reset is triggered
            if is_reset_due_to_error or is_reset:
                # Reset to previous frame as the new reference (keyframe)
                print("Resetting reference frame to previous frame.")
                C_ref = C_prev.copy()
                H_ref = H_prev.copy()
                N_ref = N_prev.copy()
                start_T_ref = start_T_ref @ np.linalg.inv(prev_T_ref)
                curr_T_ref = curr_T_prev.copy()
                is_reset = False

        if curr_T_ref is None:
            print("Warning: Could not establish valid transformation for current frame. Skipping.")
            cv2.imshow("frame", image_curr)
            key = cv2.waitKey(1)
            if key != -1:
                is_running = False
            continue

        # Update states for the next iteration
        C_prev = C_curr.copy()
        H_prev = H_curr.copy()
        N_prev = N_curr.copy()
        prev_T_ref = curr_T_ref.copy()

        # --- Display the object tracking result ---

        # æ˜¾ç¤ºå‚è€ƒå›¾åƒ
        image_l = reference.image.copy()
        cv2.putText(
            image_l,
            "Reference Frame",
            (20, 20),
            cv2.FONT_HERSHEY_SIMPLEX,
            0.5,
            (0, 255, 0),
            1,
        )

        center_start = np.array([reference.cx, reference.cy]).astype(np.int32)
        unit_vectors_start = reference.get_coordinate_system_vectors()
        annotate_coordinate_system(image_l, center_start, unit_vectors_start)

        # æ˜¾ç¤ºå½“å‰å¸§
        image_r = image_curr.copy()
        cv2.putText(
            image_r,
            "Current Frame (Tracking)",
            (20, 20),
            cv2.FONT_HERSHEY_SIMPLEX,
            0.5,
            (255, 255, 255),
            1,
        )

        center_curr = np.array([cx_curr, cy_curr]).astype(np.int32)

        # Calculate cumulative transformation
        curr_T_start = curr_T_ref @ np.linalg.inv(start_T_ref)

        # Extract rotation angle from transformation matrix
        R_curr = curr_T_start[:3, :3]
        raw_angle_z_rad = np.arctan2(R_curr[1, 0], R_curr[0, 0])

        # è®¡ç®—ç›¸å¯¹äºå‚è€ƒåæ ‡ç³»çš„è§’åº¦
        calculated_angle = raw_angle_z_rad + reference.principal_angle

        # åªåœ¨ç¬¬ä¸€å¸§è°ƒæ•´è§’åº¦åˆ°å³åŠå¹³é¢
        if first_frame_after_contact_established and not first_frame_angle_adjusted:
            adjusted_first_angle = adjust_angle_to_right_half(calculated_angle)
            angle_offset = adjusted_first_angle - calculated_angle
            angle_z_rad = adjusted_first_angle
            first_frame_after_contact_established = False
            first_frame_angle_adjusted = True
            print(f"First frame angle adjustment: raw={np.degrees(calculated_angle):.2f}Â°, adjusted={np.degrees(angle_z_rad):.2f}Â°, offset={np.degrees(angle_offset):.2f}Â°")
        else:
            # å¯¹äºåç»­å¸§ï¼Œåº”ç”¨ç›¸åŒçš„åç§»é‡ä»¥ä¿æŒè¿ç»­æ€§
            angle_z_rad = calculated_angle + angle_offset

        angle_z_deg = np.degrees(angle_z_rad)

        # è·å–ç”¨äºå¯è§†åŒ–çš„åæ ‡ç³»å‘é‡
        coord_vectors = get_coordinate_vectors_from_angle(angle_z_rad, scale=1)
        annotate_coordinate_system(image_r, center_curr, coord_vectors)

        # è®¡ç®—åœ¨å½“å‰å‚è€ƒåæ ‡ç³»ä¸­çš„ä½ç§»
        center_3d_curr = (
            np.array(
                [(cx_curr - imgw / 2 + 0.5), (cy_curr - imgh / 2 + 0.5), 0]
            )
            * ppmm / 1000.0
        )

        point = center_3d_curr[:2]

        # åº”ç”¨å‚è€ƒåæ ‡ç³»çš„å˜æ¢
        T_ref = reference.get_coordinate_transform_matrix()
        point_transformed = (T_ref[:2, :2] @ point.reshape(-1, 1)).flatten()

        # è®¡ç®—ç›´çº¿æ–¹ç¨‹
        if np.abs(np.cos(angle_z_rad)) < 1e-6:
            print(f"ç›´çº¿åœ¨å›¾åƒåæ ‡ç³»ä¸‹ä¸ºç«–çº¿: x = {point_transformed[0]:.6f}")
            # å¯¹äºç«–çº¿ï¼Œè®¾ç½®ä¸€ä¸ªå¾ˆå¤§çš„æˆªè·å€¼æ¥è¡¨ç¤ºå‚ç›´çº¿
            b = float('inf') if point_transformed[0] >= 0 else float('-inf')
        else:
            k = np.tan(angle_z_rad)
            b = point_transformed[1] - k * point_transformed[0]
            # print(f"å›¾åƒåæ ‡ç³» - æ–œç‡ k = {k:.6f}, æˆªè· b = {b:.6f}, è§’åº¦ = {angle_z_deg:.6f}Â°")

        # publish degree via ROS, publisher is ros_pub_tracking
        publish_tracking_data(ros_pub_tracking, angle_z_deg, b, center_3d_curr[0], center_3d_curr[1])

        # åœ¨å›¾åƒä¸Šæ˜¾ç¤ºè§’åº¦ä¿¡æ¯
        cv2.putText(
            image_r,
            f"Angle (angle_z_deg): {angle_z_deg:.2f}",
            (20, 50),
            cv2.FONT_HERSHEY_SIMPLEX,
            0.5,
            (255, 255, 255),
            1,
        )

        # Display combined image (å·¦è¾¹referenceï¼Œå³è¾¹current)
        cv2.imshow("frame", cv2.hconcat([image_l, image_r]))

        # Publish image_r via ROS
        print(f"[DEBUG] å‡†å¤‡å‘å¸ƒå›¾åƒï¼Œå°ºå¯¸: {image_r.shape}")
        publish_image(ros_pub_image, ros_bridge, image_r)
        
        # Publish raw image via ROS (without any annotations)
        print(f"[DEBUG] å‡†å¤‡å‘å¸ƒåŸå§‹å›¾åƒï¼Œå°ºå¯¸: {image_curr.shape}")
        publish_raw_image(ros_pub_raw_image, ros_bridge, image_curr)

        key = cv2.waitKey(1)
        if key == ord('q'):
            is_running = False
        elif key == ord('r'):
            reference = generate_reference_from_first_frame(image_curr, recon, ppmm, MIN_CONTACT_AREA_PIXELS)
        elif key == ord("i"):
            collect_background_images(device, recon)

    device.release()
    cv2.destroyAllWindows()
    print("Streaming session ended.")


if __name__ == "__main__":
    realtime_object_tracking()
