#!/usr/bin/env python3
import sys
import signal
import atexit
sys.path.append("FoundationPose")
from estimater import *
from datareader import *
from dino_mask import get_mask_from_GD 
from qwen_mask import get_mask_from_qwen
from create_camera import CreateRealsense
import cv2
import numpy as np
# import open3d as o3d
import pyrealsense2 as rs
# import torch
import time, os, sys
import json
import threading
from datetime import datetime
import gc
import torch
# from ultralytics.models.sam import Predictor as SAMPredictor
from simple_api import SimpleApi, ForceMonitor, ErrorMonitor
from dobot_gripper import DobotGripper
from transforms3d.euler import euler2mat, mat2euler
from scipy.spatial.transform import Rotation as R
import queue
from spatialmath import SE3, SO3
from grasp_utils import normalize_angle, extract_euler_zyx, print_pose_info
from calculate_grasp_pose_from_object_pose import execute_grasp_from_object_pose, detect_dent_orientation
from camera_reader import CameraReader
from level2_action import detect_object_pose_using_foundation_pose, choose_grasp_pose, execute_grasp_action
from env import create_env
import rospy
from std_msgs.msg import Float64MultiArray


camera = None
angle_camera = None
contact_camera = None
dobot = None
gripper = None
preview_running = None


def _cleanup_resources():
    """é‡Šæ”¾ç›¸æœºã€æœºæ¢°è‡‚å’Œçª—å£ç­‰èµ„æº"""
    global camera, angle_camera, contact_camera, dobot, preview_running
    try:
        if preview_running:
            preview_running.clear()
            print("[æ¸…ç†] ç›¸æœºé¢„è§ˆçº¿ç¨‹å·²åœæ­¢")
    except Exception:
        pass
    try:
        if angle_camera and getattr(angle_camera, "cap", None):
            angle_camera.cap.release()
    except Exception:
        pass
    try:
        if contact_camera and getattr(contact_camera, "cap", None):
            contact_camera.cap.release()
    except Exception:
        pass
    try:
        if camera:
            camera.release()
    except Exception:
        pass
    try:
        if dobot:
            dobot.stop()
            dobot.disable_robot()
    except Exception:
        pass
    cv2.destroyAllWindows()
def _signal_handler(signum, frame):
    print("\n[ä¸­æ–­] ç”¨æˆ·ç»ˆæ­¢ç¨‹åº")
    _cleanup_resources()
    try:
        rospy.signal_shutdown("User interrupt")
    except Exception:
        pass
    sys.exit(0)
signal.signal(signal.SIGINT, _signal_handler)
atexit.register(_cleanup_resources)




# åˆå§‹åŒ–å‡½æ•°åœ¨ env.py ä¸­


if __name__ == "__main__":
    rospy.init_node('ros_test', anonymous=True)
    # åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„ä¿å­˜ç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    save_dir = os.path.join("record_images_during_grasp", timestamp)
    os.makedirs(save_dir, exist_ok=True)
    # print(f"å›¾åƒå°†ä¿å­˜åˆ°: {save_dir}")
    angle_log_path = os.path.join(save_dir, "angle_log.csv")
    with open(angle_log_path, 'w') as f:
        f.write("frame,timestamp,angle_z_deg,detected_angles,avg_angle\n")
    # print(f"è§’åº¦æ•°æ®å°†ä¿å­˜åˆ°: {angle_log_path}")

    # ä½¿ç”¨ env.py åˆå§‹åŒ–ç¯å¢ƒï¼ˆåŒ…å«æœºæ¢°è‡‚ã€ç›¸æœºç­‰ï¼‰
    env = create_env("config.json")
    robot_main = env.robot1
    dobot = robot_main["robot"]
    gripper = env.gripper
    camera_main = env.camera1_main
    camera = camera_main["cam"]  # ä¸ºæ¸…ç†å‡½æ•°è®¾ç½®å…¨å±€å˜é‡
    T_ee_cam = camera_main["T_ee_cam"]
    
    # ä» GraspLibrary.json åŠ è½½æŠ“å–å‚æ•°
    target_object = "stirring rod"  # å¯ä»¥ä¿®æ”¹ä¸º: "red cylinder", "red stirring rod", "stirring rod"
    with open("GraspLibrary.json", 'r') as f:
        grasp_library = json.load(f)
    if target_object not in grasp_library:
        raise ValueError(f"ç›®æ ‡ç‰©ä½“ '{target_object}' ä¸åœ¨ GraspLibrary.json ä¸­")
    grasp_params = grasp_library[target_object]
    print(f"\nåŠ è½½ç›®æ ‡ç‰©ä½“: {target_object}")
    print(f"æŠ“å–å‚æ•°: {grasp_params}\n")
    
    # mesh_file = "mesh/cube.obj"
    # mesh_file = "mesh/thin_cube.obj"
    mesh_file = "mesh/cube_1_20.obj"
    # è°ƒç”¨å°è£…å¥½çš„å‡½æ•°æ£€æµ‹ç‰©ä½“ä½å§¿
    center_pose = detect_object_pose_using_foundation_pose(
        target=target_object,
        mesh_path=mesh_file,
        cam=camera_main  # ä½¿ç”¨ env åˆå§‹åŒ–çš„ camera_mainï¼ŒåŒ…å« cam å’Œ cam_k
    )


    key = cv2.waitKey(1)
    #? æ€ä¹ˆæ£€æŸ¥æ²¡æœ‰åï¼Ÿ
    angle_camera = CameraReader(camera_id=10, init_camera=True)   #! ç”¨äºè§’åº¦æ£€æµ‹çš„USBç›¸æœº (id=11, æ˜¯ååŠ çš„)
    contact_camera = CameraReader(camera_id=11, init_camera=True) #! ç”¨äºè§¦ç¢°æ£€æµ‹çš„USBç›¸æœº ï¼ˆid=10, æ˜¯åŸæ¥çš„ï¼‰


    # å°†center_poseè½¬æ¢ä¸ºnumpyæ•°ç»„
    center_pose_array = np.array(center_pose, dtype=float)
    # ä» GraspLibrary è·å–æŠ“å–å‚æ•°
    z_xoy_angle = grasp_params["z_xoy_angle"]
    vertical_euler = grasp_params["vertical_euler"]
    grasp_tilt_angle = grasp_params["grasp_tilt_angle"]
    angle_threshold = grasp_params["angle_threshold"]
    T_safe_distance = grasp_params["T_safe_distance"]
    z_safe_distance = grasp_params["z_safe_distance"]
    gripper_close_pos = grasp_params["gripper_close_pos"]
    
    # è®¡ç®—æŠ“å–å§¿æ€
    pre_grasp_pose, grasp_pose, T_base_ee_ideal = choose_grasp_pose(
        center_pose_array=center_pose_array,
        dobot=dobot,
        T_ee_cam=T_ee_cam,
        z_xoy_angle=z_xoy_angle,
        vertical_euler=vertical_euler,
        grasp_tilt_angle=grasp_tilt_angle,
        angle_threshold=angle_threshold,
        T_tcp_ee_z=-0.16,
        T_safe_distance=T_safe_distance,
        z_safe_distance=z_safe_distance,
        verbose=True
    )
    
    # æ‰§è¡ŒæŠ“å–åŠ¨ä½œ
    success = execute_grasp_action(
        grasp_pose=grasp_pose,
        dobot=dobot,
        gripper=gripper,
        gripper_close_pos=gripper_close_pos,
        move_speed=8,
        gripper_force=10,
        gripper_speed=30,
        verbose=True
    )

    wait_grasp = rospy.Rate(1.0 / 3)
    wait_grasp.sleep()
    
    pose_now = dobot.get_pose()
    x_adjustment = 15
    y_adjustment = 65
    z_adjustment = 70
    dobot.move_to_pose(pose_now[0]+x_adjustment, pose_now[1]+y_adjustment, pose_now[2]+z_adjustment, pose_now[3], pose_now[4], pose_now[5], speed=7, acceleration=1) 


#-----------å¼€å§‹æ£€æµ‹ç»ç’ƒæ£’æ–¹å‘-------------------------------------------------------
    print("\n" + "="*60)
    print("ğŸ” å¼€å§‹æ£€æµ‹ç»ç’ƒæ£’æ–¹å‘...")
    print("="*60)
    
    detected_angles = None
    avg_angle = 0.0
    detection_attempts = 0
    
    while True:
        detection_attempts += 1

        raw_image = angle_camera.get_current_frame()
        if raw_image is None:
            print(f"ç¬¬{detection_attempts}æ¬¡å°è¯•: ç­‰å¾…ç›¸æœºæ•°æ®...")
            time.sleep(0.1)
            continue
        img_timestamp = time.time()

        print(f"\nğŸ“· ç¬¬{detection_attempts}æ¬¡å°è¯•: æ£€æµ‹æ–°åŸå§‹å›¾åƒæ–¹å‘ (æ—¶é—´æˆ³: {img_timestamp:.2f})")
        detected_angles, avg_angle = detect_dent_orientation(raw_image, save_dir=save_dir)

        if detected_angles:
            last_valid_detected_angles = detected_angles
            last_valid_avg_angle = avg_angle
            last_seen_img_ts = img_timestamp
            print(f"æˆåŠŸæ£€æµ‹åˆ°ç‰©ä½“æœå‘è§’åº¦: {detected_angles}, å¹³å‡: {avg_angle:.2f}Â°, ç»å¯¹å€¼: {abs(avg_angle):.2f}Â°")
            print("="*60)
            break
        else:
            print("å½“å‰å›¾åƒæœªæ£€æµ‹åˆ°æ˜æ˜¾æ–¹å‘ç‰¹å¾ï¼Œç»§ç»­ç­‰å¾…...")
            time.sleep(0.1)

        # å¯é€‰ï¼šæœ€å¤§å°è¯•æ¬¡æ•°é™åˆ¶
        if detection_attempts >= 100:
            print(" è­¦å‘Š: è¾¾åˆ°æœ€å¤§å°è¯•æ¬¡æ•°(100æ¬¡)ï¼Œä½¿ç”¨é»˜è®¤è§’åº¦")
            detected_angles = []
            avg_angle = 0.0
            break

    wait_detect = rospy.Rate(1.0 / 1)
    wait_detect.sleep()


#-----------å¼€å§‹è°ƒæ•´ç»ç’ƒæ£’å§¿æ€-------------------------------------------------------

    print("å¼€å§‹è°ƒæ•´ç»ç’ƒæ£’å§¿æ€è‡³å‚ç›´æ¡Œé¢å‘ä¸‹")
    pose_now = dobot.get_pose()
    delta_ee = abs(avg_angle) - grasp_tilt_angle
    #éœ€è¦è®©tcpæœå¤–æ—‹è½¬ï¼› grasp_tilt_angleä¸ºæ­£å€¼æ—¶ï¼Œtcpä¼šæœå¤–æ—‹è½¬ã€‚
    x_adjustment =0
    y_adjustment =0
    z_adjustment =-15
    pose_target = [pose_now[0]+x_adjustment, pose_now[1]+y_adjustment, pose_now[2]+z_adjustment, pose_now[3]+delta_ee, pose_now[4], pose_now[5]]
    dobot.move_to_pose(pose_target[0], pose_target[1], pose_target[2], pose_target[3], pose_target[4], pose_target[5], speed=12, acceleration=1)
    

    wait_rate = rospy.Rate(1.0 / 9.0)  
    wait_rate.sleep()
    
    # éªŒè¯æ˜¯å¦åˆ°è¾¾ç›®æ ‡ä½ç½®
    pose_after_adjust = dobot.get_pose()
    print(f"æ£€æŸ¥å§¿æ€è°ƒæ•´æ˜¯å¦å®Œæˆ: Rx={pose_after_adjust[3]:.2f}Â° (ç›®æ ‡: {pose_target[3]:.2f}Â°)")
    



#-----------å¼€å§‹æ£€æµ‹ç»ç’ƒæ£’æ˜¯å¦è§¦ç¢°åˆ°æ¡Œé¢-------------------------------------------------------
    print("\nå¼€å§‹ç›‘æµ‹ç»ç’ƒæ£’ä¸æ¡Œé¢æ¥è§¦...")

    gray_debug_dir = os.path.join(save_dir, "gray_images_debug")
    os.makedirs(gray_debug_dir, exist_ok=True)
    print(f"ç°åº¦å›¾å°†ä¿å­˜åˆ°: {gray_debug_dir}")

    sample_interval = 0.1  # ç§’
    move_step = 3          # mm
    max_steps = 700
    change_threshold = 3 #0.06% å˜åŒ–çµæ•åº¦ 

    rate = rospy.Rate(1.0 / sample_interval)
    rate.sleep()
    # rospy.sleep(sample_interval)
    frame_before = None
    while frame_before is None:
        initial_frame = contact_camera.get_current_frame()
        if initial_frame is not None:
            frame_before = initial_frame
        else:
            print("ç­‰å¾…åˆå§‹å›¾åƒ...")
            rospy.sleep(sample_interval)

    print("å·²è·å–åˆå§‹å›¾åƒ")
    pose_current = dobot.get_pose()

    for step in range(max_steps):
        wait = rospy.Rate(33)  
        wait.sleep()
        # åŠ¨ä½œå‰å¸§
        frame_data_before = contact_camera.get_current_frame()
        if frame_data_before is None:
            print(f"  æ­¥éª¤ {step+1}: ç­‰å¾…åŠ¨ä½œå‰å›¾åƒ...")
            rospy.sleep(sample_interval)
            continue
        frame_before = frame_data_before

        # å‘ä¸‹ç§»åŠ¨ä¸€å°æ­¥
        pose_current[2] -= move_step
        dobot.move_to_pose(
            pose_current[0], pose_current[1], pose_current[2],
            pose_current[3], pose_current[4], pose_current[5],
            speed=5, acceleration=1
        )

        # ç­‰å¾…å¹¶æŠ“å–åŠ¨ä½œåçš„æ–°å¸§
        frame_after = None
        has_change = False
        #è¿ç»­é«˜é¢‘é‡‡æ ·æ£€æµ‹
        for _ in range(20): #0.1*20 = 2s
            rate.sleep()
            candidate_frame = contact_camera.get_current_frame()
            if candidate_frame is not None:
                frame_after = candidate_frame

                has_change = contact_camera.has_significant_change(
                    frame_before, frame_after,
                    change_threshold=change_threshold,
                    pixel_threshold=2,
                    min_area=2,
                    save_dir=gray_debug_dir,
                    step_num=step
                )

                if has_change:
                    break
            
                # break

        if frame_after is None:
            print(f"  æ­¥éª¤ {step+1}: æœªæ”¶åˆ°æ–°å›¾åƒï¼Œç»§ç»­ç­‰å¾…...")
            continue


        if has_change:
            print(f"æ£€æµ‹åˆ°æ˜¾è‘—å˜åŒ–ï¼ç»ç’ƒæ£’å¯èƒ½å·²æ¥è§¦æ¡Œé¢ (æ­¥æ•°: {step+1}, ä¸‹é™: {(step+1)*move_step}mm)")
            break

        print(f"  æ­¥éª¤ {step+1}/{max_steps}: æœªæ£€æµ‹åˆ°æ¥è§¦ï¼Œç»§ç»­ä¸‹é™...")
    else:
        print("è¾¾åˆ°å‚ç›´å‘ä¸‹æœ€å¤§ç§»åŠ¨è·ç¦»ï¼Œæœªæ£€æµ‹åˆ°æ˜æ˜¾å˜åŒ–")

    print("ç»ç’ƒæ£’ä¸‹é™æ£€æµ‹å®Œæˆ\n")

        
    # å¯é€‰ï¼šè¿”å›homeä½ç½®ï¼ˆæ ¹æ®éœ€è¦å–æ¶ˆæ³¨é‡Šï¼‰
    # dobot.move_to_pose(435.4503, 281.809, 348.9125, -179.789, -0.8424, 14.4524, speed=9)


